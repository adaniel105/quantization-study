## quanitization-study
Studying quanitization techniques & parameter efficient fine tuning methods to reduce the computational overhead of training and inference with multi-million/billion parameter models.

Quantization techniques:
- Overview : https://www.inferless.com/learn/quantization-techniques-demystified-boosting-efficiency-in-large-language-models-llms
- GGUF: https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/

## Paper Backlog

- QLoRA: Efficient Finetuning of quantized LLMs - https://arxiv.org/abs/2305.14314
- GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection - https://arxiv.org/abs/2403.03507
- The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits: https://arxiv.org/abs/2402.17764
- LISA: Layerwise Importance Sampling for Memory Efficient Large Language Model Fine-Tuning - https://arxiv.org/abs/2403.17919

